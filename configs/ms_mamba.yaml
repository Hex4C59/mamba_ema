# MS-Mamba: Multi-Source Adaptive Sliding-Mamba for SER
# Uses frame-level LLD features, processes all frames (no keyframe selection)

experiment:
  name: ms_mamba_iemocap
  base_dir: runs

data:
  name: IEMOCAP
  use_offline_features: true
  use_lld_features: true  # Enable frame-level LLD mode

  params:
    label_file: data/labels/IEMOCAP/iemocap_label.csv
    feature_root: data/features/IEMOCAP  # Contains wavlm/, ecapa/, egemaps_lld/
    fold: 1
    normalize_vad: false

  loader:
    batch_size: 16
    shuffle: true
    num_workers: 4

model:
  name: MSMamba
  params:
    # Input dimensions
    d_speech: 1024          # WavLM-Large hidden size (no reduction)
    d_prosody_in: 25        # eGeMAPS LLD dimension
    d_speaker: 192          # ECAPA-TDNN dimension

    # Projection dimensions
    d_prosody_proj: 128     # Small projection for LLDs only
    d_hidden: 1152          # = d_speech + d_prosody_proj, no compression

    # Learnable layer fusion
    num_wavlm_layers: 4     # Number of WavLM layers (12, 18, 21, 24)

    # Speaker-Adaptive LayerNorm
    use_sa_ln: true
    sa_ln_hidden: 128

    # Windowed Local Mamba
    local_mamba:
      window_size: 32
      n_layers: 1
      d_model: 1152         # No compression
      d_state: 16
      d_conv: 4
      expand: 2

    # Global Mamba (processes all frames)
    global_mamba:
      n_layers: 2
      d_model: 1152         # No compression
      d_state: 16
      d_conv: 4
      expand: 2
      pooling: mean         # mean, last, or max

    # Speaker-Aware Cross-Attention
    speaker_attn:
      num_heads: 4

    # General
    dropout: 0.2

loss:
  name: CCC

metric:
  name: CCC

train:
  epochs: 50
  seed: 42
  gpu_id: 0

  optimizer:
    name: AdamW
    lr: 0.0001
    weight_decay: 0.0001

  scheduler:
    name: CosineAnnealing
    T_max: 50

  grad_clip: 1.0

  early_stopping:
    patience: 10
    min_delta: 0.001
